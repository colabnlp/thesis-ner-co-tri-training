{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "class Preprocessor:\n",
    "    def __init__(self, filename):\n",
    "        self.row_isw_data = self.load_isw_tsv_file(filename)\n",
    "        self.cleaned_isw_data = self.clean_isw_data()\n",
    "\n",
    "    def load_isw_tsv_file(self, filename='data/test-full-isw-release.tsv'):\n",
    "        isw_data = pd.read_csv(filename, quotechar='\"',\n",
    "                               delimiter=\"\\t\", skiprows=None)\n",
    "        print(\"Total number of rows\", len(isw_data))\n",
    "        print(\"Total number of sentences\", len(isw_data.fileid.unique()))\n",
    "        return isw_data\n",
    "\n",
    "    def clean_isw_data(self, selected_cols=[]):\n",
    "        \"\"\"\n",
    "        :return: clean isw_data\n",
    "        \"\"\"\n",
    "        # Keep only selected cols\n",
    "        selected_cols = ['fileid', 'token', 'lemma', 'ontoNer']\n",
    "        isw_set = self.row_isw_data[selected_cols]\n",
    "\n",
    "        # Clean up incorrect rows  e.g. fileid -> total 82 of it\n",
    "        isw_set = isw_set[isw_set.fileid != \"fileid\"]\n",
    "\n",
    "        # Drop empty token\n",
    "        isw_drop_non = isw_set[isw_set.lemma != \"NONE\"]\n",
    "        isw_drop_non.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        # Replace NONE tag with \"O\"\n",
    "        isw_drop_non['ontoNer'].replace(\n",
    "            to_replace='NONE', value='O', inplace=True)\n",
    "        return isw_drop_non\n",
    "\n",
    "    def get_list_of_sentences(self):\n",
    "        \"\"\"\n",
    "        return : list of sentences : ['I have apple', 'I am here', 'hello ']\n",
    "        \"\"\"\n",
    "        data = self.cleaned_isw_data\n",
    "        # Group the sentence with its fileid\n",
    "        agg_func = lambda s: [(token, lem, ner) for token, lem, ner in zip(s[\"token\"].values.tolist(),\n",
    "                                                    s[\"lemma\"].values.tolist(),\n",
    "                                                    s[\"ontoNer\"].values.tolist())]\n",
    "        grouped = data.groupby(\"fileid\").apply(agg_func)\n",
    "        grouped_all = [s for s in grouped]\n",
    "\n",
    "        sentences = [\" \".join([s[0] for s in sent]) for sent in grouped_all]\n",
    "        return sentences\n",
    "\n",
    "    def get_list_of_nerlabels(self):\n",
    "        \"\"\"\n",
    "        return : list of labels : ['O', 'O', 'B-GPE', ...]\n",
    "        \"\"\"\n",
    "        data = self.cleaned_isw_data\n",
    "        # Group the sentence with its fileid\n",
    "        agg_func = lambda s: [(token, lem, ner) for token, lem, ner in zip(s[\"token\"].values.tolist(),\n",
    "                                                    s[\"lemma\"].values.tolist(),\n",
    "                                                    s[\"ontoNer\"].values.tolist())]\n",
    "        grouped = data.groupby(\"fileid\").apply(agg_func)\n",
    "        grouped_all = [s for s in grouped]\n",
    "\n",
    "        labels = [[s[2] for s in label] for label in grouped_all]\n",
    "\n",
    "        return labels\n",
    "\n",
    "    def get_tag2idx(self):\n",
    "        \"\"\"\n",
    "        return : dict of ner label with idx : {'B-ADD': 0, 'B-AGE': 1, 'B-ART': 2, 'B-CARDINAL': 3,'B-CREAT': 4, ...}\n",
    "        \"\"\"\n",
    "        data = self.cleaned_isw_data\n",
    "        # ners_vals : list of ner labels\n",
    "        ners_vals = list(set(data[\"ontoNer\"].values))\n",
    "        # Set as dict {key:idx}\n",
    "        tag2idx = {t: i for i, t in enumerate(sorted(ners_vals))}\n",
    "        return tag2idx\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# filename = 'data/test-full-isw-release.tsv'\n",
    "# pre = Preprocessor(filename)\n",
    "\n",
    "# sentences = pre.get_list_of_sentences()\n",
    "# labels = pre.get_list_of_nerlabels()\n",
    "\n",
    "# print(labels[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from seqeval.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "\n",
    "from transformers import BertTokenizer, BertForTokenClassification\n",
    "\n",
    "\n",
    "\n",
    "def get_hyperparameters(model, ff):\n",
    "\n",
    "    # ff: full_finetuning\n",
    "    if ff:\n",
    "        param_optimizer = list(model.named_parameters())\n",
    "        no_decay = [\"bias\", \"gamma\", \"beta\"]\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                \"params\": [\n",
    "                    p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n",
    "                ],\n",
    "                \"weight_decay_rate\": 0.01,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [\n",
    "                    p for n, p in param_optimizer if any(nd in n for nd in no_decay)\n",
    "                ],\n",
    "                \"weight_decay_rate\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "    else:\n",
    "        param_optimizer = list(model.classifier.named_parameters())\n",
    "        optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
    "\n",
    "    return optimizer_grouped_parameters\n",
    "\n",
    "def flat_accuracy(preds, labels):\n",
    "    return np.sum(np.array(preds)==np.array(labels))/len(labels)\n",
    "\n",
    "def annot_confusion_matrix(valid_tags, pred_tags):\n",
    "\n",
    "    \"\"\"\n",
    "    Create an annotated confusion matrix by adding label\n",
    "    annotations and formatting to sklearn's `confusion_matrix`.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create header from unique tags\n",
    "    header = sorted(list(set(valid_tags + pred_tags)))\n",
    "\n",
    "    # Calculate the actual confusion matrix\n",
    "    matrix = confusion_matrix(valid_tags, pred_tags, labels=header)\n",
    "\n",
    "    # Final formatting touches for the string output\n",
    "    mat_formatted = [header[i] + \"\\t\" + str(row) for i, row in enumerate(matrix)]\n",
    "    content = \"\\t\" + \" \".join(header) + \"\\n\" + \"\\n\".join(mat_formatted)\n",
    "\n",
    "    return content\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check if it works ...the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "FILE_NAME = \"data/test-full-isw-release.tsv\"\n",
    "BERT_MODEL = \"bert-base-german-cased\"\n",
    "MAX_LEN = 75\n",
    "BATCH_SIZE = 32\n",
    "FULL_FINETUNING = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory for storing our model checkpoints\n",
    "# if not os.path.exists(\"/models\"):\n",
    "#     os.mkdir(\"/models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Devices available: cpu\n"
     ]
    }
   ],
   "source": [
    "# Specify device data for training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Devices available: {}\".format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize PRETRAINED TOKENIZER\n",
    "tokenizer = BertTokenizer.from_pretrained(BERT_MODEL, do_lower_case=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows 300684\n",
      "Total number of sentences 84\n"
     ]
    }
   ],
   "source": [
    "# Load preprocessed sentences, labels and tag2idx\n",
    "pre = Preprocessor(filename=FILE_NAME)\n",
    "\n",
    "sentences = pre.get_list_of_sentences()\n",
    "labels = pre.get_list_of_nerlabels()\n",
    "# Create dicts for mapping from labels to IDs and back\n",
    "tag2idx = pre.get_tag2idx()\n",
    "idx2tag = {i: t for t, i in tag2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83\n",
      "83\n",
      "['O', 'O', 'O', 'O', 'O', 'B-TIME', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DUR', 'O', 'B-GPE', 'O', 'O', 'O', 'O', 'B-SORD', 'I-SORD', 'O', 'O', 'O', 'O', 'O', 'O', 'B-GPE', 'O', 'B-TIME', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FREQ', 'O', 'O', 'O', 'O', 'O', 'B-AGE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-TIME', 'O', 'O', 'B-AGE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-TIME', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-TIME', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FAC', 'O', 'O', 'O', 'B-TIME', 'O', 'O', 'O', 'O', 'O', 'B-GPE', 'O', 'O', 'O', 'O', 'O', 'O', 'B-GPE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-GPE', 'I-GPE', 'B-FAC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FAC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ADD', 'I-ADD', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-TITLE', 'B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NRP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-CARDINAL', 'B-NRP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NRP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-TIME', 'O', 'O', 'B-TIME', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DATE', 'O', 'B-DATE', 'O', 'O', 'B-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DATE', 'O', 'O', 'O', 'O', 'B-DUR', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DUR', 'I-DUR', 'B-TIME', 'O', 'O', 'B-NRP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NRP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FAC', 'I-FAC', 'O', 'O', 'O', 'O', 'B-DUR', 'I-DUR', 'O', 'O', 'O', 'B-TIME', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-GPE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-TIME', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORDINAL', 'O', 'O', 'O', 'O', 'B-DATE', 'B-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DUR', 'I-DUR', 'O', 'O', 'O', 'O', 'B-DUR', 'I-DUR', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-TIME', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-RATE', 'I-RATE', 'I-RATE', 'I-RATE', 'O', 'B-RATE', 'I-RATE', 'I-RATE', 'I-RATE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-CARDINAL', 'O', 'O', 'B-CARDINAL', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NRP', 'O', 'O', 'O', 'B-NRP', 'O', 'O', 'B-NRP', 'O', 'O', 'B-NRP', 'O', 'B-NRP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-TIME', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-TIME', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-SORD', 'O', 'O', 'O', 'O', 'O', 'B-DATE', 'O', 'O', 'O', 'O', 'B-DUR', 'I-DUR', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NRP', 'B-TIME', 'O', 'B-GPE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NRP', 'O', 'O', 'O', 'B-NRP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-AGE', 'O', 'O', 'B-DATE', 'O', 'O', 'O', 'O', 'B-AGE', 'O', 'B-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NRP', 'O', 'O', 'O', 'O', 'B-NRP', 'O', 'O', 'O', 'B-TIME', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NRP', 'O', 'O', 'O', 'B-NRP', 'O', 'O', 'O', 'B-DATE', 'O', 'B-NRP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DATE', 'O', 'O', 'O', 'O', 'O', 'B-CARDINAL', 'O', 'O', 'O', 'O', 'B-CARDINAL', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NRP', 'O', 'O', 'O', 'O', 'O', 'B-NRP', 'O', 'B-NRP', 'O', 'O', 'O', 'O', 'B-NRP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LAN', 'O', 'O', 'O', 'B-LAN', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DUR', 'I-DUR', 'O', 'B-DUR', 'I-DUR', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LAN', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DATE', 'B-DATE', 'O', 'O', 'B-DUR', 'I-DUR', 'O', 'O', 'B-LOC', 'O', 'B-LAN', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LAN', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-TIME', 'O', 'B-GPE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-TIME', 'O', 'O', 'O', 'B-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'B-GPE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-GPE', 'I-GPE', 'O', 'O', 'O', 'O', 'O', 'B-NRP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-GPE', 'O', 'O', 'O', 'O', 'O', 'O', 'B-GPE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-TIME', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-GPE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-GPE', 'B-DATE', 'B-DATE', 'O', 'B-GPE', 'O', 'O', 'B-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-TIME', 'O', 'O', 'O', 'B-DATE', 'B-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'B-TIME', 'B-DATE', 'O', 'B-GPE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-GPE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-GPE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DATE', 'O', 'O', 'O', 'O', 'O', 'B-GPE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-GPE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NRP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-GPE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-CARDINAL', 'O', 'O', 'O', 'O', 'O', 'B-CARDINAL', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-CARDINAL', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-GPE', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'B-GPE', 'O', 'O', 'B-GPE', 'O', 'O', 'O', 'O', 'B-GPE', 'O', 'O', 'O', 'O', 'B-EVT', 'O', 'O', 'O', 'B-TIME', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DUR', 'I-DUR', 'O', 'O', 'O', 'O', 'B-DUR', 'B-DUR', 'O', 'O', 'O', 'O', 'B-GPE', 'O', 'O', 'O', 'O', 'O', 'O', 'B-TIME', 'O', 'O', 'B-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FREQ', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MED', 'O', 'O', 'O', 'O', 'B-MED', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LAN', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FREQ', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LAN', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LAN', 'O', 'O', 'O', 'O', 'B-LAN', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ART', 'I-ART', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LAN', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DUR', 'I-DUR', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LAN', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DUR', 'I-DUR', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LAN', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LAN', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LAN', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LAN', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LAN', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LAN', 'I-LAN', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-SORD', 'I-SORD', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NRP', 'O', 'O', 'O', 'B-NRP', 'O', 'O', 'O', 'B-LAN', 'I-LAN', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-SORD', 'I-SORD', 'O', 'O', 'O', 'B-NRP', 'O', 'O', 'O', 'B-NRP', 'O', 'B-FREQ', 'I-FREQ', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DUR', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FREQ', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FREQ', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-TITLE', 'B-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NRP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DUR', 'O', 'O', 'O', 'B-DATE', 'O', 'O', 'B-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-TIME', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DUR', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-TIME', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-TIME', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-TIME', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-TIME', 'O', 'O', 'O', 'O', 'O']\n",
      "Also Sie haben uns ja jetzt schon sehr viel Interessantes erzählt und von Ihrer Jugendzeit in Wien könnten Sie uns da noch einmal ihre Eindrücke vermitteln Meine Eindrücke von Wien von damals Also ich ich glaub ich hatte nur gute ehrlich gestanden Worauf führ ich das zurück Ich kann mich erinnern dass man mich gefragt hat Na was willst denn werden Das ist doch eine sehr aktuelle Frage immer für Kinder und ich war zwölf und ich sehe mich bei meiner Tante die das eben auch fragt und ich sag Jetzt bin ich zwölf eigentlich würd ich ganz gern so bleiben Und das ist eigentlich ich wenn so ein Kind sagt so was ohne sich darüber den Kopf zu zerbrechen warum sagt sie n das eigentlich Aber ich kann mich dran erinnern also muss ich doch zufrieden gewesen sein sonst hätt ich das nicht gesagt Und ich bin ä Jetzt sag ich was was ich nicht sagen sollte Denn ich bin nur in die Hauptschule gegangen Es hat Leute ja schon gegeben das brauchst doch niemand sagen Sag ich Aber warum soll ich das nicht sagen Ich hab solche Ich ich bin viel zu ehrlich Ja ich bin bestimmt viel zu ehrlich Aber ich bin eben viel zu ehrlich und jetzt bin ich alte Frau und da wer ich mich nicht mehr ändern Und ich will mich vielleicht auch nicht ändern Also ich bin wirklich nur in die Hauptschule gegangen und zwar in der in der Schulgasse Die Schule besteht heute noch Sie kennen sich in Wien nicht aus Sie kennen sich in Wien aus Mh ein bisschen also Ja ja es ist egal in welchem Bezirk im 18 Bezirk Schulgasse ja und äh die Volksschule war übrigens auch in der Schulgasse aber dieses Haus steht nicht mehr Aber dies die Schule v in der Die Hauptschule in der Schulgasse 59 die steht sehr wohl noch und wir hatten eine sehr gute einen sehr guten Klassenvorstand und zwar die Frau Emma Schrammek Und ich glaub sie hat äh Sie war nicht antisemitisch Wir haben es jedenfalls nicht gemerkt wir waren aber nur zwei jüdische Kinder in der Klasse Und das ist schon irgendwie etwas Bedeutendes Die Juden haben ihre Kinder nicht in die Volks in die Hauptschule geschickt sondern ins Gymnasium Alle Alle meine Freundinnen die ich außerhalb der Schule hatte die waren im Gymnasium Und warum man mich nicht ins Gymnasium geschickt hat hat verschiedene Gründe Ob ich die Aufnahmsprüfung nicht bestanden hätte das weiß ich nicht das kann ich heut nicht sagen Ich hab nicht besonders gern gelernt das weiß ich Aber meine Eltern hatten auch nicht die Mitteln dazu Das hat ja viel Geld gekostet Und dann hat man dann eben in der Familie gesagt Sie soll halt an Beruf lernen Ich glaub ich war damit einverstanden wie man das so hinge schmissen hat irgendwie Aber ich habs nicht geschafft sogar die Hauptschule fertig zu machen Denn wie gesagt ich bin vierundzwanzig geboren achtunddreißig ist der Hitler gekommen und ich bin auch im November geboren sodass ich ein Jahr zu spät in die Schule eingetreten bin Ich hatte nur sieben Schuljahre Dann wurden alle jüdische Kinder in von der ganzen Stadt ich glaube von der ganzen Stadt in eine jüdische Schule äh umgeschult die war in der Währinger Straße Und da bin ich paar Monate noch gegangen aber dann war Schluss auch damit war Schluss Ja also jedenfalls bin ich mit der Jugend Alija das ist eine eine Jugend gruppenorganisation gewesen ich glaub von Amerika aus ist das gegangen Aber jedenfalls äh musste man da konnte man da die Kinder anmelden und irgendwie ist man dann eben hier hergekommen In verschiedene Kibbuzim ist man verteilt worden Ich glaube nicht dass man in der Stadt auch Kinder aufgenommen hat Und in einem Kibbuz ist das eben nicht wie Sie waren in einem Kibbuz schon auf Besuch Nein Nein Da müssen Sie aber fahren Werden wir werden wir ja Mhm Ja also das Der Kibbuz ist aber nicht was er heute ist Das war was ganz anderes Denn erstens war das wie gesagt neununddreißig vierzig und das war Es war es war goar nichts also es war sehr viel aber es es war doch Krieg Unt hat Niemand hat viel zu essen gehabt Und uns hat man auch nicht viel zu essen gegeben Und die Jugend Alija war so eingeteilt dass sie einen halben Tag lernen soll und einen halben Tag arbeiten soll Am Feld oder was eben war Im Kinderhaus die Kinder haben damals nicht zuhaus geschlafen die Kinder waren alle separiert von den Eltern Das war Da müssen Sie das lesen wirklich da da gibts noch und noch zu erzählen Das war wie ein Kolchos fast oder vielleicht sogar mehr in manchen Sachen Die Eltern haben gearbeitet und die Kinder sind im Kinderhaus groß geworden Sie hatten Kontakt mit den Eltern oder die Eltern mit den Kindern aber eine Stunde am Tag oder zwei Stunden am Tag Das war so eingeteilt Es gab aber verschiedene Kibbuzim mit verschiedenen ideologischen Richtungen Die haben halt jä Es ist immer dasselbe bei fünf Menschen sind fünf verschiedene Meinungen bei allen ob das bei Juden ist oder bei Christen oder bei Evangelischen also bei Katholiken oder Evangelen ist des is nun mal so dass man Menschen sehr schwer in ein und dieselbe Schachtel tun kann Das geht nicht Und in den Kibbuzim war genau dasselbe Also in dem Kibbuz in dem ich war da sind die Kinder wirklich eh am Nachmittag zu den Eltern gekommen wenn die von der Arbeit zurückgekommen sind Und en Am Abend haben die Eltern die Kinder hingelegt Aber die Kinder haben zusammen geschlafen und die Eltern wieder das ist auch Das gibts heut gar nicht mehr Schon paar Jahre das ist abgeschafft worden Die Kinder wohnen zuhaus Des es Man ist draufgekommen dass das falsch war Ja also das war der Kibbuz und da war ich eben plötzlich hebräisch Vorher in Wien haben Sie Also sind Sie vorbereitet worden In der Zionistik Ich bi wir sind ein bissel schon vorbe Also wir haben gewusst was sich tut Wir waren se Wir waren Idealisten Wir wurden als Idealisten erzogen Das kann man ja bei den Jugendlichen Ich glaub heute nicht Ich glaub die Jugend von heute lässt sich nicht idealisisieren Nein nicht nur hier nicht oder bei Ihnen nicht Es ist nicht mehr modern idealistisch zu sein Man muss materialistisch sein Das war damals out das das war nicht Das war wirklich nicht man war Idealist Man war stolz Idealist zu sein Wenn heut einer Idealist ist muss er sich fast schämen Hm Ja Stimmts Stimmt Muss sich heut fast schämen Das war nicht so Das war wirklich die Ich will nicht behaupten dass alle Wieviel waren in diesem Kibbuz wenn ich das heute mir so überleg wieviel Menschen 100 vielleicht ungefähr vielleicht auch 120 Ich weiß nicht so genau aber ungefähr in diesem Maß Das waren Idealisten Das waren zum Teil aber Polen und Russen Es waren auch einige Deutsche dabei aber sehr wenige Und wir sollten von heut auf morgen nur noch Hebräisch reden und nicht Deutsch Und das war natürlich nicht leicht Und es war nicht nur nicht leicht es hat uns auch gestört selbstverständlich Denn also was wir Wir haben in einem Extra Haus gewohnt Nicht mit denen zusammen also nicht mit den Mitgliedern des Kibbuzes wir waren ja nicht Mitglied wir waren ja nur geduldet Wir mussten zwar halben Tag arbeiten halben Tag lernen hat man uns einen Lehrer gegeben aber wir sollten kein Deutsch reden Aber man darf nicht vergessen unter welchen Umständen war denn das vierzig einundvierzig Ich war zweieinhalb Jahre in dem Kibbuz Das Deutsch reden war verpönt Und es war genug Grund dazu da Denn alle hatten irgendwelche Verwandten irgendwo und hatten keine Ahnung was mit denen passiert Das war schon äh bedingt Aber wir haben trotzdem Deutsch geredet natürlich untereinander das ist klar Und vom Kibbuz bin ich dann nach Benjamina gegangen selbstständig das ist ein kleines Dorf damals gewesen das gibts heut auch noch in der Nähe von Zichron wenn Sie zufällig wird man Ihnen vielleicht zeigen die die Weinkellereien von Zichron Ja akow Das könnt möglich sein Österreicher zeigt man gern die Weinkellereien Und in Benjamina sind auch aber mehr noch in Zichron das ist also äh ein kleines Dorf gewesen und dort habe ich eben gearbeitet als Dienstmädchen ich hab Ich musste doch endlich einmal was lernen Und hab als Dienstmädchen gearbeitet und hab das Geld gespart um nach Jerusalem zu gehen und Säuglingsschwester zu lernen das war mein Ziel Und das hab ich auch irgendwie durchgeführt mal schlechter mal weniger Äh mal schlechter also schwieriger mal weniger schwierig und bin eigentlich nach Jerusalem fünfundvierzig Fünfundvierzig Nach Jerusalem bin ich fünfundvierzig gekommen und hab Säuglingsschwester gelernt Nun ja und dann haben wir geheiratet siemundvierzig siemundvierzig habe ich geheiratet und wir sind dann fünfzig nach Ecuador gegangen weil die Eltern meines Mannes in Ecuador Weil man nirgendst hingehen konnte ist man eben nach Ecuador gegangen und die haben dort ein Buchgeschäft aufgemacht und mein Mann hat dort gearbeitet in diesem Buchgeschäft und ich hab einen Sohn bekommen einundfünfzig und hab gesagt Ich gehe nicht arbeiten ich hab doch ein Kind Ich bin äh irgendwie noch ziemlich oldfashioned kön Ja auf jeden Fall das weiß ich Bin eben äh 51 haben schon auch auch in Ecuador haben schon Frauen gesagt sie wollen gar nicht zuhaus bleiben Außerdem in Ecuador darf man nicht vergessen da kann man ein Dienstmädel haben für für fast kein Geld Die südamerikanischen Verhältnisse sind total anders als als hier und auch als in Europa das ist so vielleicht hat sichs bisserl verändert in der Zwischenzeit Aber ich glaube nicht Das ist noch ziemlich so geblieben Dass diese Das Dienstpersonal ist sehr billig Und die Viele Damen haben das sehr ausgenützt Die haben drei Mädeln im Haus gehabt nicht eine Und ich wollt das nicht Ich hab gesagt Wozu brauch ich da drei Mädel dann bin ich doch gar mehr d gehör ich gar nicht mehr dazu Also das sind private Ansichten Auf jeden Fall Nun gut und wir haben gewusst also da wir doch von hier gekommen sind wir sind ja nicht mehr von Europa gekommen nach Südamerika Wir sind ja von Palästina also von Israel das war ja schon Israel Dazwischen war der LACHEND Unabhängigkeitskrieg den hab ich jetzt ganz vergessen da warn wa beim Militär beide Also nicht mit Schießgewehr aber mit eben beim Militär im Büro und so verschiedene Ich war ein Jahr beim Militär mein Mann länger anderthalb Das war vor vor Ecuador Ja und was fehlt noch und dann sind wir sechzig zurückgekommen und so sind wir seit sechzig hier MURMELT Und da warn auch verschie So was man so erlebt eben Überall jeder Mensch hat eine Geschichte auf jeden Fall nein Jeder Mensch hat eine Geschichte ganz egal wo er lebt und wie er lebt Einmal ist die Geschichte sehr lustig und schön und manchmal ist sie eben nicht ganz so lustig Das gehört dazu das gehört zum Leben Aber wenn Sie Sprachwissenschaftler sind dann hab ich äh sehr viel äh zu zu fragen Wenn ich Ich hab doch hier Kabel Und ich sehe den 3Sat und ich sehe den Sat1 und so weiter Das ist doch eine ganz komische Sprache heute Denn was da für Worte hineinkommen vom Englischen das ist doch Ich mein ich hab nichts dagegen ich ich wohn ja auch gar nicht dort ich hab ja gar keine Berechtigung eigentlich Aber oft denk ich mir Warum muss das so sein S ist äh Deutsch ist doch eine ziemlich reich Zum Beispiel Iwrit muss ich Ihnen sagen Hebräisch ist eine arme Sprache Wirklich also äh wahrscheinlich in der Thora und und die heilige Sprache die heilige Schrift ist nicht arm Aber das ist ja nicht die Umgangssprache Die Umgangssprache ist arm Und es gibt kaum ein Wort ff also für ein Wort verschiedene Bezeichnungen sondern immer nur eine oder mehr oder weniger fast nur eine Hebräisch ist eine arme Sprache Wenn einer Ihnen sagt Das ist doch nicht wahr Es ist leider wahr Oder Gott sei Dank Ich mein warum muss eine Sprache Warum warum ist eine Sprache reich Weil sie sich anreichert von außen Sie reichert sich doch an von außen wenn sie lebt Und diese Sprache war doch zweitausend Jahr tot Das darf man nicht vergessen Man hat auch gesagt Man wird hier nie Hebräisch sprechen können in der Umgangssprache weil sie eben zweitausend Jahre tot war In äh im Gebet und die ganz Frommen die sprechen ja nicht Hebräisch das haben Sie schon wahrscheinlich Die reden nur Jiddisch Und die ich weiß nicht wie das ist in den Schulen von denen Da bin ich überfragt und denn mich mich Ich bin mit ihnen nicht sehr sehr zufrieden In keiner Weise Denn so wie sie leben und so wie sie sind das können sie überall Äh heut wird sie keiner herausschmeißen aus keinem Land Man wird sie dulden man wird sich nicht für sehr freuen vielleicht aber man wird sie dulden Und wenn sie hier sind in ihrem Land dann haben sie sich doch eigentlich an dieses Land anzupassen Das tun sie ja nicht das tun sie gar nicht Das wollen sie nicht Angefangen von der Sprache Ich glaub ja die können alle Hebräisch aber sie werden zuhaus bestimmt nicht Hebräisch sprechen Und die Kinder reden auch alle alle Jiddisch Aber das ist doch ein anderes Problem Das ist doch was für die Politiker Müssen sie studieren political science Ja ja wirklich ich glaube es im Ernst Und obs unsere Sprache sich verändert hat is logisch Aber ich glaube sagen zu dürfen dass die deutsche Sprache sich genauso verändert hat ohne uns Wir haben damit gar nichts zu tun Ich hab ein Wort gibt es das immer wieder vorkommt Und ich überhaupt nicht weiß was ich damit machen soll Warum benützen die Deutschen und auch die Österreicher also in der deutschen Sprache das Wort schlechthin Schlechthin hab ich mir aufgeschrieben was es nämlich bedeutet Weil ich das so komisch find Das heißt durchaus Das heißt also irgend eine Verstärkung ja Aber warum schlecht Schlecht hat doch eine ganz andere Bedeutung Und ich hör das immer wieder sowohl von den Österreichern als von den Deutschen Und jedes Mal gibts mir einen einen Ruck Zu meiner Zeit hats das Wort nicht gegeben Bestimmt nicht Wie wie wie als LACHEND Sprachenwissenschaftler was machen Sie mit LACHEND dem Wort Das muss doch von irgendwo kommen warum schlecht Schlecht ist negativ und durchaus ist positiv also wie geht das LACHEND Sie ind sprachlos LACHT Sprechen Sprachlos ja LACHT Obwohl ich nicht Sprachen studiert hab und i Obwohl alle sagen Sag nicht du hast keine Matura Sage ich Ich sag sehr NACHDRÜCKLICH wohl Ich hab keine Matura Ich bin ja nicht selber schuld dran Also kann ich das ruhig sagen Aber mir fallen solche Sachen sofort auf Ich glaube gerade wenn man ah mehrere Sprachen lernt mehrere Sprachen spricht achtet man besonders auf solche Ich MURMELND weiß nicht aber ich bin genug gehört Ich weiß es nicht ich Oft denke ich es gibt schon Leute die dasselbe Schicksal haben wie ich Und trotzdem haben sies geschafft äh irgendeinen akademischen Grad zu zu erlernen oder zu bekommen eben Ich hab das nicht gekonnt I Ich will mich nicht entschuldigen im Gegenteil Ich äh bin mit mir nicht ganz einverstanden dass ichs nicht geschafft hab oder gar nicht versucht hab Sagen wa mal so Denn es gibt Abendschulen und es gibt noch und noch äh äh Gelegenheiten und äh ich habs ich Ich weiß aber wieso ich sie nicht ausnutzen konnte ich musste ja immer Geld verdienen ich musste ja von irgendwas leben Die andern sind ja mit Eltern hergekommen In meiner Generation Also es gibt ganz verschiedene Generationen Es gibt Generationen Haben Sie schon irgendwie eine Verbindung gehabt mit da Frau Goldschmidt Denn die hat das irgendwie unter sich gehabt Diese Interviews sind bei ihr abgegeben worden ja Ja ja Also die die die hat da was damit zu tun Sie ist sie ist Vorsitzende von den österreichischen Pansionisten Hier im Land Und sie brüstet sich immer sie hat doch schon äh weiß ich wieviel Universitäts äh abs jahre gehabt wie der acht ah in achtunddreißig Na ich kann ja nichts dafür dass ich später auf die Welt gekommen bin Denn die die ist älter als ich Nicht schrecklich viel aber immerhin in in so einer Situation spielt doch jedes Jahr eine Rolle Und sie redt dann immer davon und so weiter Also gut sie darf reden was sie will Aber aber dann denk ich mir immer Warum muss ich mir Vorwürfe machen Ich mach mir aber Vorwürfe denn es gibt wirklich manche die wahnsinnig wissensdurstig waren Ich war nicht wissich wissens durstig und ich weiß auch warum Weil ich aus einer Generation komm wo die Eltern gesagt haben Frag nicht soviel Also man sitzt am Tisch und da ist Besuch oder so irgendwas Na jetzt hör schon auf frag nicht so viel Das gibts ja heute nicht Und es hats vielleicht auch nicht in jeder Familie gegeben damals Das kann auch sein Das kann ich nicht beurteilen Aber das ist bestimmt mit ein Grund gewesen Da hat doch so ein Kind eine Vorstellung Das ist nicht schön wenn man so viel fragt Und äh was ist Wissensdurst Ist Fragen Man will wissen Also das war das und ich sag jetzt werd ich schon nix nachholen\n"
     ]
    }
   ],
   "source": [
    "print(len(labels))\n",
    "print(len(sentences))\n",
    "print(labels[0])\n",
    "print(sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of NER tags: 60\n"
     ]
    }
   ],
   "source": [
    "print('number of NER tags:', len(tag2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the sentences\n",
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "\n",
    "# Get input id of token\n",
    "# mapped fixed sentence size\n",
    "# if len of sent less than max len -> add word with 0 index to match the max len (padding)\n",
    "# if one word is exisiting with several blocks, it will be assigned with same index\n",
    "input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n",
    "                          maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "# Map tags of labels with input_ids\n",
    "# Get tags of labels with id\n",
    "tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in labels],\n",
    "                     maxlen=MAX_LEN, value=tag2idx[\"O\"], padding=\"post\",\n",
    "                     dtype=\"long\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Also',\n",
       " 'Sie',\n",
       " 'haben',\n",
       " 'uns',\n",
       " 'ja',\n",
       " 'jetzt',\n",
       " 'schon',\n",
       " 'sehr',\n",
       " 'viel',\n",
       " 'Interess',\n",
       " '##antes',\n",
       " 'erzählt',\n",
       " 'und',\n",
       " 'von',\n",
       " 'Ihrer',\n",
       " 'Jugend',\n",
       " '##zeit',\n",
       " 'in',\n",
       " 'Wien',\n",
       " 'könnten',\n",
       " 'Sie',\n",
       " 'uns',\n",
       " 'da',\n",
       " 'noch',\n",
       " 'einmal',\n",
       " 'ihre',\n",
       " 'Ein',\n",
       " '##drücke',\n",
       " 'vermitteln',\n",
       " 'Meine',\n",
       " 'Ein',\n",
       " '##drücke',\n",
       " 'von',\n",
       " 'Wien',\n",
       " 'von',\n",
       " 'damals',\n",
       " 'Also',\n",
       " 'ich',\n",
       " 'ich',\n",
       " 'glaub',\n",
       " 'ich',\n",
       " 'hatte',\n",
       " 'nur',\n",
       " 'gute',\n",
       " 'ehr',\n",
       " '##lich',\n",
       " 'gestanden',\n",
       " 'Wor',\n",
       " '##auf',\n",
       " 'führ',\n",
       " 'ich',\n",
       " 'das',\n",
       " 'zurück',\n",
       " 'Ich',\n",
       " 'kann',\n",
       " 'mich',\n",
       " 'erinnern',\n",
       " 'dass',\n",
       " 'man',\n",
       " 'mich',\n",
       " 'gefragt',\n",
       " 'hat',\n",
       " 'Na',\n",
       " 'was',\n",
       " 'will',\n",
       " '##st',\n",
       " 'denn',\n",
       " 'werden',\n",
       " 'Das',\n",
       " 'ist',\n",
       " 'doch',\n",
       " 'eine',\n",
       " 'sehr',\n",
       " 'aktuelle',\n",
       " 'Frage',\n",
       " 'immer',\n",
       " 'für',\n",
       " 'Kinder',\n",
       " 'und',\n",
       " 'ich',\n",
       " 'war',\n",
       " 'zwölf',\n",
       " 'und',\n",
       " 'ich',\n",
       " 'sehe',\n",
       " 'mich',\n",
       " 'bei',\n",
       " 'meiner',\n",
       " 'Tante',\n",
       " 'die',\n",
       " 'das',\n",
       " 'eben',\n",
       " 'auch',\n",
       " 'fragt',\n",
       " 'und',\n",
       " 'ich',\n",
       " 'sa',\n",
       " '##g',\n",
       " 'Jetzt',\n",
       " 'bin',\n",
       " 'ich',\n",
       " 'zwölf',\n",
       " 'eigentlich',\n",
       " 'wür',\n",
       " '##d',\n",
       " 'ich',\n",
       " 'ganz',\n",
       " 'gern',\n",
       " 'so',\n",
       " 'bleiben',\n",
       " 'Und',\n",
       " 'das',\n",
       " 'ist',\n",
       " 'eigentlich',\n",
       " 'ich',\n",
       " 'wenn',\n",
       " 'so',\n",
       " 'ein',\n",
       " 'Kind',\n",
       " 'sagt',\n",
       " 'so',\n",
       " 'was',\n",
       " 'ohne',\n",
       " 'sich',\n",
       " 'darüber',\n",
       " 'den',\n",
       " 'Kopf',\n",
       " 'zu',\n",
       " 'zer',\n",
       " '##brechen',\n",
       " 'warum',\n",
       " 'sagt',\n",
       " 'sie',\n",
       " 'n',\n",
       " 'das',\n",
       " 'eigentlich',\n",
       " 'Aber',\n",
       " 'ich',\n",
       " 'kann',\n",
       " 'mich',\n",
       " 'dran',\n",
       " 'erinnern',\n",
       " 'also',\n",
       " 'muss',\n",
       " 'ich',\n",
       " 'doch',\n",
       " 'zufrieden',\n",
       " 'gewesen',\n",
       " 'sein',\n",
       " 'sonst',\n",
       " 'hät',\n",
       " '##t',\n",
       " 'ich',\n",
       " 'das',\n",
       " 'nicht',\n",
       " 'gesagt',\n",
       " 'Und',\n",
       " 'ich',\n",
       " 'bin',\n",
       " 'ä',\n",
       " 'Jetzt',\n",
       " 'sa',\n",
       " '##g',\n",
       " 'ich',\n",
       " 'was',\n",
       " 'was',\n",
       " 'ich',\n",
       " 'nicht',\n",
       " 'sagen',\n",
       " 'sollte',\n",
       " 'Denn',\n",
       " 'ich',\n",
       " 'bin',\n",
       " 'nur',\n",
       " 'in',\n",
       " 'die',\n",
       " 'Haupt',\n",
       " '##schule',\n",
       " 'gegangen',\n",
       " 'Es',\n",
       " 'hat',\n",
       " 'Leute',\n",
       " 'ja',\n",
       " 'schon',\n",
       " 'gegeben',\n",
       " 'das',\n",
       " 'brauch',\n",
       " '##st',\n",
       " 'doch',\n",
       " 'niemand',\n",
       " 'sagen',\n",
       " 'Sa',\n",
       " '##g',\n",
       " 'ich',\n",
       " 'Aber',\n",
       " 'warum',\n",
       " 'soll',\n",
       " 'ich',\n",
       " 'das',\n",
       " 'nicht',\n",
       " 'sagen',\n",
       " 'Ich',\n",
       " 'hab',\n",
       " 'solche',\n",
       " 'Ich',\n",
       " 'ich',\n",
       " 'bin',\n",
       " 'viel',\n",
       " 'zu',\n",
       " 'ehr',\n",
       " '##lich',\n",
       " 'Ja',\n",
       " 'ich',\n",
       " 'bin',\n",
       " 'bestimmt',\n",
       " 'viel',\n",
       " 'zu',\n",
       " 'ehr',\n",
       " '##lich',\n",
       " 'Aber',\n",
       " 'ich',\n",
       " 'bin',\n",
       " 'eben',\n",
       " 'viel',\n",
       " 'zu',\n",
       " 'ehr',\n",
       " '##lich',\n",
       " 'und',\n",
       " 'jetzt',\n",
       " 'bin',\n",
       " 'ich',\n",
       " 'alte',\n",
       " 'Frau',\n",
       " 'und',\n",
       " 'da',\n",
       " 'wer',\n",
       " 'ich',\n",
       " 'mich',\n",
       " 'nicht',\n",
       " 'mehr',\n",
       " 'ändern',\n",
       " 'Und',\n",
       " 'ich',\n",
       " 'will',\n",
       " 'mich',\n",
       " 'vielleicht',\n",
       " 'auch',\n",
       " 'nicht',\n",
       " 'ändern',\n",
       " 'Also',\n",
       " 'ich',\n",
       " 'bin',\n",
       " 'wirklich',\n",
       " 'nur',\n",
       " 'in',\n",
       " 'die',\n",
       " 'Haupt',\n",
       " '##schule',\n",
       " 'gegangen',\n",
       " 'und',\n",
       " 'zwar',\n",
       " 'in',\n",
       " 'der',\n",
       " 'in',\n",
       " 'der',\n",
       " 'Schul',\n",
       " '##gasse',\n",
       " 'Die',\n",
       " 'Schule',\n",
       " 'besteht',\n",
       " 'heute',\n",
       " 'noch',\n",
       " 'Sie',\n",
       " 'kennen',\n",
       " 'sich',\n",
       " 'in',\n",
       " 'Wien',\n",
       " 'nicht',\n",
       " 'aus',\n",
       " 'Sie',\n",
       " 'kennen',\n",
       " 'sich',\n",
       " 'in',\n",
       " 'Wien',\n",
       " 'aus',\n",
       " 'M',\n",
       " '##h',\n",
       " 'ein',\n",
       " 'bisschen',\n",
       " 'also',\n",
       " 'Ja',\n",
       " 'ja',\n",
       " 'es',\n",
       " 'ist',\n",
       " 'egal',\n",
       " 'in',\n",
       " 'welchem',\n",
       " 'Bezirk',\n",
       " 'im',\n",
       " '18',\n",
       " 'Bezirk',\n",
       " 'Schul',\n",
       " '##gasse',\n",
       " 'ja',\n",
       " 'und',\n",
       " 'ä',\n",
       " '##h',\n",
       " 'die',\n",
       " 'Volksschule',\n",
       " 'war',\n",
       " 'übrigens',\n",
       " 'auch',\n",
       " 'in',\n",
       " 'der',\n",
       " 'Schul',\n",
       " '##gasse',\n",
       " 'aber',\n",
       " 'dieses',\n",
       " 'Haus',\n",
       " 'steht',\n",
       " 'nicht',\n",
       " 'mehr',\n",
       " 'Aber',\n",
       " 'dies',\n",
       " 'die',\n",
       " 'Schule',\n",
       " 'v',\n",
       " 'in',\n",
       " 'der',\n",
       " 'Die',\n",
       " 'Haupt',\n",
       " '##schule',\n",
       " 'in',\n",
       " 'der',\n",
       " 'Schul',\n",
       " '##gasse',\n",
       " '59',\n",
       " 'die',\n",
       " 'steht',\n",
       " 'sehr',\n",
       " 'wohl',\n",
       " 'noch',\n",
       " 'und',\n",
       " 'wir',\n",
       " 'hatten',\n",
       " 'eine',\n",
       " 'sehr',\n",
       " 'gute',\n",
       " 'einen',\n",
       " 'sehr',\n",
       " 'guten',\n",
       " 'Klassen',\n",
       " '##vorstand',\n",
       " 'und',\n",
       " 'zwar',\n",
       " 'die',\n",
       " 'Frau',\n",
       " 'Emma',\n",
       " 'Schr',\n",
       " '##amme',\n",
       " '##k',\n",
       " 'Und',\n",
       " 'ich',\n",
       " 'glaub',\n",
       " 'sie',\n",
       " 'hat',\n",
       " 'ä',\n",
       " '##h',\n",
       " 'Sie',\n",
       " 'war',\n",
       " 'nicht',\n",
       " 'antisemit',\n",
       " '##isch',\n",
       " 'Wir',\n",
       " 'haben',\n",
       " 'es',\n",
       " 'jedenfalls',\n",
       " 'nicht',\n",
       " 'gem',\n",
       " '##erkt',\n",
       " 'wir',\n",
       " 'waren',\n",
       " 'aber',\n",
       " 'nur',\n",
       " 'zwei',\n",
       " 'jüdische',\n",
       " 'Kinder',\n",
       " 'in',\n",
       " 'der',\n",
       " 'Klasse',\n",
       " 'Und',\n",
       " 'das',\n",
       " 'ist',\n",
       " 'schon',\n",
       " 'irgendwie',\n",
       " 'etwas',\n",
       " 'Bedeut',\n",
       " '##endes',\n",
       " 'Die',\n",
       " 'Juden',\n",
       " 'haben',\n",
       " 'ihre',\n",
       " 'Kinder',\n",
       " 'nicht',\n",
       " 'in',\n",
       " 'die',\n",
       " 'Volks',\n",
       " 'in',\n",
       " 'die',\n",
       " 'Haupt',\n",
       " '##schule',\n",
       " 'geschickt',\n",
       " 'sondern',\n",
       " 'ins',\n",
       " 'Gymnasium',\n",
       " 'Alle',\n",
       " 'Alle',\n",
       " 'meine',\n",
       " 'Freundin',\n",
       " '##nen',\n",
       " 'die',\n",
       " 'ich',\n",
       " 'außerhalb',\n",
       " 'der',\n",
       " 'Schule',\n",
       " 'hatte',\n",
       " 'die',\n",
       " 'waren',\n",
       " 'im',\n",
       " 'Gymnasium',\n",
       " 'Und',\n",
       " 'warum',\n",
       " 'man',\n",
       " 'mich',\n",
       " 'nicht',\n",
       " 'ins',\n",
       " 'Gymnasium',\n",
       " 'geschickt',\n",
       " 'hat',\n",
       " 'hat',\n",
       " 'verschiedene',\n",
       " 'Gründe',\n",
       " 'Ob',\n",
       " 'ich',\n",
       " 'die',\n",
       " 'Aufnahm',\n",
       " '##sprüfung',\n",
       " 'nicht',\n",
       " 'bestanden',\n",
       " 'hätte',\n",
       " 'das',\n",
       " 'weiß',\n",
       " 'ich',\n",
       " 'nicht',\n",
       " 'das',\n",
       " 'kann',\n",
       " 'ich',\n",
       " 'heut',\n",
       " 'nicht',\n",
       " 'sagen',\n",
       " 'Ich',\n",
       " 'hab',\n",
       " 'nicht',\n",
       " 'besonders',\n",
       " 'gern',\n",
       " 'gelernt',\n",
       " 'das',\n",
       " 'weiß',\n",
       " 'ich',\n",
       " 'Aber',\n",
       " 'meine',\n",
       " 'Eltern',\n",
       " 'hatten',\n",
       " 'auch',\n",
       " 'nicht',\n",
       " 'die',\n",
       " 'Mitteln',\n",
       " 'dazu',\n",
       " 'Das',\n",
       " 'hat',\n",
       " 'ja',\n",
       " 'viel',\n",
       " 'Geld',\n",
       " 'gek',\n",
       " '##ost',\n",
       " '##et',\n",
       " 'Und',\n",
       " 'dann',\n",
       " 'hat',\n",
       " 'man',\n",
       " 'dann',\n",
       " 'eben',\n",
       " 'in',\n",
       " 'der',\n",
       " 'Familie',\n",
       " 'gesagt',\n",
       " 'Sie',\n",
       " 'soll',\n",
       " 'halt',\n",
       " 'an',\n",
       " 'Beruf',\n",
       " 'lernen',\n",
       " 'Ich',\n",
       " 'glaub',\n",
       " 'ich',\n",
       " 'war',\n",
       " 'damit',\n",
       " 'einverstanden',\n",
       " 'wie',\n",
       " 'man',\n",
       " 'das',\n",
       " 'so',\n",
       " 'hin',\n",
       " '##ge',\n",
       " 'schm',\n",
       " '##issen',\n",
       " 'hat',\n",
       " 'irgendwie',\n",
       " 'Aber',\n",
       " 'ich',\n",
       " 'hab',\n",
       " '##s',\n",
       " 'nicht',\n",
       " 'geschafft',\n",
       " 'sogar',\n",
       " 'die',\n",
       " 'Haupt',\n",
       " '##schule',\n",
       " 'fertig',\n",
       " 'zu',\n",
       " 'machen',\n",
       " 'Denn',\n",
       " 'wie',\n",
       " 'gesagt',\n",
       " 'ich',\n",
       " 'bin',\n",
       " 'vier',\n",
       " '##undzwanzig',\n",
       " 'geboren',\n",
       " 'acht',\n",
       " '##und',\n",
       " '##drei',\n",
       " '##ßig',\n",
       " 'ist',\n",
       " 'der',\n",
       " 'Hitler',\n",
       " 'gekommen',\n",
       " 'und',\n",
       " 'ich',\n",
       " 'bin',\n",
       " 'auch',\n",
       " 'im',\n",
       " 'November',\n",
       " 'geboren',\n",
       " 'sodass',\n",
       " 'ich',\n",
       " 'ein',\n",
       " 'Jahr',\n",
       " 'zu',\n",
       " 'spät',\n",
       " 'in',\n",
       " 'die',\n",
       " 'Schule',\n",
       " 'eingetreten',\n",
       " 'bin',\n",
       " 'Ich',\n",
       " 'hatte',\n",
       " 'nur',\n",
       " 'sieben',\n",
       " 'Schuljahr',\n",
       " '##e',\n",
       " 'Dann',\n",
       " 'wurden',\n",
       " 'alle',\n",
       " 'jüdische',\n",
       " 'Kinder',\n",
       " 'in',\n",
       " 'von',\n",
       " 'der',\n",
       " 'ganzen',\n",
       " 'Stadt',\n",
       " 'ich',\n",
       " 'glaube',\n",
       " 'von',\n",
       " 'der',\n",
       " 'ganzen',\n",
       " 'Stadt',\n",
       " 'in',\n",
       " 'eine',\n",
       " 'jüdische',\n",
       " 'Schule',\n",
       " 'ä',\n",
       " '##h',\n",
       " 'umge',\n",
       " '##schul',\n",
       " '##t',\n",
       " 'die',\n",
       " 'war',\n",
       " 'in',\n",
       " 'der',\n",
       " 'Wä',\n",
       " '##hr',\n",
       " '##inger',\n",
       " 'Straße',\n",
       " 'Und',\n",
       " 'da',\n",
       " 'bin',\n",
       " 'ich',\n",
       " 'paar',\n",
       " 'Monate',\n",
       " 'noch',\n",
       " 'gegangen',\n",
       " 'aber',\n",
       " 'dann',\n",
       " 'war',\n",
       " 'Schluss',\n",
       " 'auch',\n",
       " 'damit',\n",
       " 'war',\n",
       " 'Schluss',\n",
       " 'Ja',\n",
       " 'also',\n",
       " 'jedenfalls',\n",
       " 'bin',\n",
       " 'ich',\n",
       " 'mit',\n",
       " 'der',\n",
       " 'Jugend',\n",
       " 'Ali',\n",
       " '##ja',\n",
       " 'das',\n",
       " 'ist',\n",
       " 'eine',\n",
       " 'eine',\n",
       " 'Jugend',\n",
       " 'g',\n",
       " '##ruppen',\n",
       " '##organisation',\n",
       " 'gewesen',\n",
       " 'ich',\n",
       " 'glaub',\n",
       " 'von',\n",
       " 'Amerika',\n",
       " 'aus',\n",
       " 'ist',\n",
       " 'das',\n",
       " 'gegangen',\n",
       " 'Aber',\n",
       " 'jedenfalls',\n",
       " 'ä',\n",
       " '##h',\n",
       " 'musste',\n",
       " 'man',\n",
       " 'da',\n",
       " 'konnte',\n",
       " 'man',\n",
       " 'da',\n",
       " 'die',\n",
       " 'Kinder',\n",
       " 'anmel',\n",
       " '##den',\n",
       " 'und',\n",
       " 'irgendwie',\n",
       " 'ist',\n",
       " 'man',\n",
       " 'dann',\n",
       " 'eben',\n",
       " 'hier',\n",
       " 'her',\n",
       " '##gekommen',\n",
       " 'In',\n",
       " 'verschiedene',\n",
       " 'Ki',\n",
       " '##bb',\n",
       " '##uz',\n",
       " '##im',\n",
       " 'ist',\n",
       " 'man',\n",
       " 'verteilt',\n",
       " 'worden',\n",
       " 'Ich',\n",
       " 'glaube',\n",
       " 'nicht',\n",
       " 'dass',\n",
       " 'man',\n",
       " 'in',\n",
       " 'der',\n",
       " 'Stadt',\n",
       " 'auch',\n",
       " 'Kinder',\n",
       " 'aufgenommen',\n",
       " 'hat',\n",
       " 'Und',\n",
       " 'in',\n",
       " 'einem',\n",
       " 'Ki',\n",
       " '##bb',\n",
       " '##uz',\n",
       " 'ist',\n",
       " 'das',\n",
       " 'eben',\n",
       " 'nicht',\n",
       " 'wie',\n",
       " 'Sie',\n",
       " 'waren',\n",
       " 'in',\n",
       " 'einem',\n",
       " 'Ki',\n",
       " '##bb',\n",
       " '##uz',\n",
       " 'schon',\n",
       " 'auf',\n",
       " 'Besuch',\n",
       " 'Nein',\n",
       " 'Nein',\n",
       " 'Da',\n",
       " 'müssen',\n",
       " 'Sie',\n",
       " 'aber',\n",
       " 'fahren',\n",
       " 'Werden',\n",
       " 'wir',\n",
       " 'werden',\n",
       " 'wir',\n",
       " 'ja',\n",
       " 'M',\n",
       " '##hm',\n",
       " 'Ja',\n",
       " 'also',\n",
       " 'das',\n",
       " 'Der',\n",
       " 'Ki',\n",
       " '##bb',\n",
       " '##uz',\n",
       " 'ist',\n",
       " 'aber',\n",
       " 'nicht',\n",
       " 'was',\n",
       " 'er',\n",
       " 'heute',\n",
       " 'ist',\n",
       " 'Das',\n",
       " 'war',\n",
       " 'was',\n",
       " 'ganz',\n",
       " 'anderes',\n",
       " 'Denn',\n",
       " 'ersten',\n",
       " '##s',\n",
       " 'war',\n",
       " 'das',\n",
       " 'wie',\n",
       " 'gesagt',\n",
       " 'neun',\n",
       " '##und',\n",
       " '##drei',\n",
       " '##ßig',\n",
       " 'vierzig',\n",
       " 'und',\n",
       " 'das',\n",
       " 'war',\n",
       " 'Es',\n",
       " 'war',\n",
       " 'es',\n",
       " 'war',\n",
       " 'g',\n",
       " '##o',\n",
       " '##ar',\n",
       " 'nichts',\n",
       " 'also',\n",
       " 'es',\n",
       " 'war',\n",
       " 'sehr',\n",
       " 'viel',\n",
       " 'aber',\n",
       " 'es',\n",
       " 'es',\n",
       " 'war',\n",
       " 'doch',\n",
       " 'Krieg',\n",
       " 'Un',\n",
       " '##t',\n",
       " 'hat',\n",
       " 'Niemand',\n",
       " 'hat',\n",
       " 'viel',\n",
       " 'zu',\n",
       " 'essen',\n",
       " 'gehabt',\n",
       " 'Und',\n",
       " 'uns',\n",
       " 'hat',\n",
       " 'man',\n",
       " 'auch',\n",
       " 'nicht',\n",
       " 'viel',\n",
       " 'zu',\n",
       " 'essen',\n",
       " 'gegeben',\n",
       " 'Und',\n",
       " 'die',\n",
       " 'Jugend',\n",
       " 'Ali',\n",
       " '##ja',\n",
       " 'war',\n",
       " 'so',\n",
       " 'eingeteilt',\n",
       " 'dass',\n",
       " 'sie',\n",
       " 'einen',\n",
       " 'halben',\n",
       " 'Tag',\n",
       " 'lernen',\n",
       " 'soll',\n",
       " 'und',\n",
       " 'einen',\n",
       " 'halben',\n",
       " 'Tag',\n",
       " 'arbeiten',\n",
       " 'soll',\n",
       " 'Am',\n",
       " 'Feld',\n",
       " 'oder',\n",
       " 'was',\n",
       " 'eben',\n",
       " 'war',\n",
       " 'Im',\n",
       " 'Kinder',\n",
       " '##haus',\n",
       " 'die',\n",
       " 'Kinder',\n",
       " 'haben',\n",
       " 'damals',\n",
       " 'nicht',\n",
       " 'zu',\n",
       " '##haus',\n",
       " 'gesch',\n",
       " '##lafen',\n",
       " 'die',\n",
       " 'Kinder',\n",
       " 'waren',\n",
       " 'alle',\n",
       " 'separ',\n",
       " '##iert',\n",
       " 'von',\n",
       " 'den',\n",
       " 'Eltern',\n",
       " 'Das',\n",
       " 'war',\n",
       " 'Da',\n",
       " 'müssen',\n",
       " 'Sie',\n",
       " 'das',\n",
       " 'lesen',\n",
       " 'wirklich',\n",
       " 'da',\n",
       " 'da',\n",
       " 'gibt',\n",
       " '##s',\n",
       " 'noch',\n",
       " 'und',\n",
       " 'noch',\n",
       " 'zu',\n",
       " 'erzählen',\n",
       " 'Das',\n",
       " 'war',\n",
       " 'wie',\n",
       " 'ein',\n",
       " 'Kol',\n",
       " '##ch',\n",
       " '##os',\n",
       " 'fast',\n",
       " 'oder',\n",
       " 'vielleicht',\n",
       " 'sogar',\n",
       " 'mehr',\n",
       " 'in',\n",
       " 'manchen',\n",
       " 'Sachen',\n",
       " 'Die',\n",
       " 'Eltern',\n",
       " 'haben',\n",
       " 'gearbeitet',\n",
       " 'und',\n",
       " 'die',\n",
       " 'Kinder',\n",
       " 'sind',\n",
       " 'im',\n",
       " 'Kinder',\n",
       " '##haus',\n",
       " 'groß',\n",
       " 'geworden',\n",
       " 'Sie',\n",
       " 'hatten',\n",
       " 'Kontakt',\n",
       " 'mit',\n",
       " 'den',\n",
       " 'Eltern',\n",
       " 'oder',\n",
       " 'die',\n",
       " 'Eltern',\n",
       " 'mit',\n",
       " 'den',\n",
       " 'Kindern',\n",
       " 'aber',\n",
       " 'eine',\n",
       " 'Stunde',\n",
       " 'am',\n",
       " 'Tag',\n",
       " 'oder',\n",
       " 'zwei',\n",
       " 'Stunden',\n",
       " 'am',\n",
       " 'Tag',\n",
       " 'Das',\n",
       " 'war',\n",
       " 'so',\n",
       " 'eingeteilt',\n",
       " 'Es',\n",
       " 'gab',\n",
       " 'aber',\n",
       " 'verschiedene',\n",
       " 'Ki',\n",
       " '##bb',\n",
       " '##uz',\n",
       " '##im',\n",
       " 'mit',\n",
       " 'verschiedenen',\n",
       " 'ide',\n",
       " '##ologischen',\n",
       " 'Richtungen',\n",
       " 'Die',\n",
       " 'haben',\n",
       " 'halt',\n",
       " 'j',\n",
       " '##ä',\n",
       " 'Es',\n",
       " 'ist',\n",
       " 'immer',\n",
       " 'dasselbe',\n",
       " 'bei',\n",
       " 'fünf',\n",
       " 'Menschen',\n",
       " 'sind',\n",
       " 'fünf',\n",
       " 'verschiedene',\n",
       " 'Meinung',\n",
       " '##en',\n",
       " 'bei',\n",
       " 'allen',\n",
       " 'ob',\n",
       " 'das',\n",
       " 'bei',\n",
       " 'Juden',\n",
       " 'ist',\n",
       " 'oder',\n",
       " 'bei',\n",
       " 'Christen',\n",
       " 'oder',\n",
       " 'bei',\n",
       " 'Evangelischen',\n",
       " 'also',\n",
       " 'bei',\n",
       " 'Katholiken',\n",
       " 'oder',\n",
       " 'Evangel',\n",
       " '##en',\n",
       " 'ist',\n",
       " 'des',\n",
       " 'is',\n",
       " 'nun',\n",
       " 'mal',\n",
       " 'so',\n",
       " 'dass',\n",
       " 'man',\n",
       " 'Menschen',\n",
       " 'sehr',\n",
       " 'schwer',\n",
       " 'in',\n",
       " 'ein',\n",
       " 'und',\n",
       " 'dieselbe',\n",
       " 'Schacht',\n",
       " '##el',\n",
       " 'tun',\n",
       " 'kann',\n",
       " 'Das',\n",
       " 'geht',\n",
       " 'nicht',\n",
       " 'Und',\n",
       " 'in',\n",
       " 'den',\n",
       " 'Ki',\n",
       " '##bb',\n",
       " '##uz',\n",
       " '##im',\n",
       " 'war',\n",
       " 'genau',\n",
       " 'dasselbe',\n",
       " 'Also',\n",
       " ...]"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(83, 75)"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[59, 59, 59, ..., 59, 59,  1],\n",
       "       [59, 59, 59, ..., 59, 59, 59],\n",
       "       [59, 59, 59, ..., 59, 59, 59],\n",
       "       ...,\n",
       "       [59, 59, 59, ..., 59, 59,  8],\n",
       "       [59, 12, 59, ..., 59, 59,  2],\n",
       "       [59, 59, 59, ..., 59, 59, 59]])"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get attention mask from BERT label\n",
    "attention_masks = [[float(i>0) for i in ii] for ii in input_ids]\n",
    "\n",
    "# Split the dataset for 10% validation\n",
    "tr_inputs, val_inputs, tr_tags, val_tags = train_test_split(input_ids, tags, \n",
    "                                                        random_state=42, test_size=0.1)\n",
    "tr_masks, val_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
    "                                                        random_state=42, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_masks[0][5990:6000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_masks[0][2000:2010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded training and validation data into DataLoaders.\n"
     ]
    }
   ],
   "source": [
    "# Convert dataset to torch tensors\n",
    "tr_inputs = torch.tensor(tr_inputs)\n",
    "val_inputs = torch.tensor(val_inputs)\n",
    "tr_tags = torch.tensor(tr_tags)\n",
    "val_tags = torch.tensor(val_tags)\n",
    "tr_masks = torch.tensor(tr_masks)\n",
    "val_masks = torch.tensor(val_masks)\n",
    "\n",
    "# Load and prepare data, define dataloaders\n",
    "# Concatenate attention mask and inputs/tags\n",
    "train_data = TensorDataset(tr_inputs, tr_masks, tr_tags)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "valid_data = TensorDataset(val_inputs, val_masks, val_tags)\n",
    "valid_sampler = SequentialSampler(valid_data)\n",
    "valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=BATCH_SIZE)\n",
    "print(\"Loaded training and validation data into DataLoaders.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "\n",
      "[tensor([[  188,   320,    18,  ...,   717,   188,  5956],\n",
      "        [ 6405,    88, 12737,  ...,  3277,   834,  1169],\n",
      "        [ 6405,    88,  7003,  ...,   456,   188, 24999],\n",
      "        ...,\n",
      "        [26718,   371,  1120,  ..., 20023,  1671, 14154],\n",
      "        [   32,   437,   295,  ...,  4537, 19009,   153],\n",
      "        [  125,  1062, 26905,  ..., 14335,   188,   181]]), tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        ...,\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.]]), tensor([[28, 59, 59,  ..., 28, 59, 59],\n",
      "        [59, 59, 21,  ..., 35, 59, 59],\n",
      "        [59, 59, 21,  ..., 59, 59, 59],\n",
      "        ...,\n",
      "        [59, 59, 59,  ..., 59, 59, 59],\n",
      "        [59, 59, 59,  ..., 59, 59, 59],\n",
      "        [59, 59, 59,  ..., 59, 59, 59]])]\n",
      "1\n",
      "\n",
      "[tensor([[13086,  4299,   229,  ...,  1169, 19009,  1671],\n",
      "        [ 5072, 19009,  1169,  ...,    50,    21,  1786],\n",
      "        [ 1356,   371,   636,  ..., 14932,  4493,  7997],\n",
      "        ...,\n",
      "        [  946,  5017,     6,  ...,  4563,   761, 11614],\n",
      "        [ 3598,  1520,   232,  ...,   287,   114,  4647],\n",
      "        [  233,  6892,   185,  ...,   128,   193,    21]]), tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        ...,\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.]]), tensor([[59, 59, 59,  ..., 59, 59, 59],\n",
      "        [28, 59, 59,  ..., 59, 59, 59],\n",
      "        [59, 59, 59,  ..., 59, 59, 59],\n",
      "        ...,\n",
      "        [29, 21, 59,  ..., 59, 59, 59],\n",
      "        [59, 59, 59,  ..., 59, 59, 59],\n",
      "        [59, 59, 59,  ..., 59, 59, 59]])]\n",
      "2\n",
      "\n",
      "[tensor([[ 1671,  3627,   357,   446,   705,   115,    86,  4588,   246,   371,\n",
      "          1913,  3934,   287,   181,  2774,    30,  1139,   417,    30,   371,\n",
      "           348, 11763,   474,   125,  1139,   417,    30,  1139,   417,   185,\n",
      "          1062, 26905,  9431,  9933,   982,  1062,   501,  3507,  2230,  1062,\n",
      "         26905,  5152,   114,  1835,  1761,   650, 10202,   127,   982,  3096,\n",
      "          1084,   478,  1011, 22210,    43,  4531,   478,  2168,   702,   155,\n",
      "         10896,  9569,   474,  1356,   670,  1062, 26905,   670,   193,    67,\n",
      "          6403, 12482,    93,   292,   155],\n",
      "        [ 1356,   670,   312, 26911,   232,  2099,  5043,   114,   128,  2230,\n",
      "           115,    21,  1621,   178,    21,  1621,  7051,    42,    93,   185,\n",
      "          1120, 20790,   948,  2617,   185,  1120, 20790,   948,    42,   232,\n",
      "           185, 26898,  1120,  1120, 20790,   948,   255,   232,   204,    30,\n",
      "          4276,   188,  2904,   883,  3484,    42,   670,   115,  1757,    30,\n",
      "          4819, 15632,   185, 26106, 26958, 26941, 26943,    42,   348,  1561,\n",
      "           232,    50,    39,  1062, 26905,  1062, 26905, 24816,  8394,   691,\n",
      "           115,   382,  3216,  1726,  7404],\n",
      "        [12482,   232,   287,  1868,  3934,  3340,  3133,  1054,   387,    29,\n",
      "         17509,  7687,   482,  3250,  3667,    67,   193,  6403,    81,  3667,\n",
      "          3507,  8672, 26900,  3969, 17030,   149,   229,   185,    30,  3240,\n",
      "          2421,   155,  5035,    88,    46, 26907, 22271,  6988,     7,   188,\n",
      "         24999,   594,   620,  1942, 20161,   163,  7593,    88,    21,   483,\n",
      "            33,    88,   534,  6642,  7081,    30,   474,    93,  9572,  1671,\n",
      "           185,   194,  2421,    39,  1357,  1169,  1084, 14991,   260,  2249,\n",
      "           193,   478,   896,  9572,   312],\n",
      "        [ 1731, 24567,   478,    50,    21,  3173,    30,  1068,    50,    21,\n",
      "          3173, 10881,   764,    30, 14090,   125, 14090,   292,   181,   827,\n",
      "          5473,   702,    50,    21,  3173,   246,    39, 13382,  2046,    50,\n",
      "          2472,  1356,  1062, 26905,    30,  4247,    21, 14090,   127,  1575,\n",
      "          2406,  1630,  1638,   155,  8754,    33,   155,  2426,  3299,    60,\n",
      "          8086,  4247,   746,   193,    67,  1856,  5034,  6306,   177,   193,\n",
      "          1856,  5034,  6306,  1671, 24925, 26908,  1881,   193,    93, 11282,\n",
      "            43,  1969,   573,  7512,  7691],\n",
      "        [ 1356,   702,   232,   636, 20189,    42, 14244,   232,   287,   702,\n",
      "           386,  3764,   185,   229,   149,   194,   702,   149,   311,  1208,\n",
      "         19009, 26898,   232,   987,  4371,  2119,   935,  7495,    42,  2197,\n",
      "           149,    93,   961,   232,  4137,  3096,  2119,  2040,  3278,  2111,\n",
      "          9881,   149,  3934,    25,   345,  1045, 26901,  1169,    50,  2319,\n",
      "          8759, 10665,    42,  1169,   185,   200,  1208,    50,    21,  9697,\n",
      "           210,   104,    86,  9697,  1120,  2519,    51, 11645,   295,  2368,\n",
      "           494,  4053,  1169,   149,   380],\n",
      "        [  295,  1302,    50, 10997,   265,    85,   295,   127,    50, 10997,\n",
      "           265,    85,   295,   193,  1020,   344,  2085, 18921,  2072,  4468,\n",
      "           498,  1139,  4468,   746,   127,  1835,  1120, 13769, 14602,  1340,\n",
      "           702,  3767, 26898,   202,   229,   104,    93,  1355,   702,  1671,\n",
      "         19009,    93,  1113, 13460,  1731,   702, 20568, 26898,   202,    93,\n",
      "           702,   104,   295,    50,   798,  1848,   193,    67,   149, 13460,\n",
      "            93,   193,   478,  9739,   128,   128,   128,    88,    21,  2698,\n",
      "            21,  2087,  6392,  6802,   193],\n",
      "        [ 7774, 10976,    85,    44,    35,   474,   232, 15652,  1346, 20879,\n",
      "            88,   297,  4727, 26049, 21046,   147, 10768,   555,  1169,    93,\n",
      "           149,  8876,   386,  1138,   479,  1169,  2265,   961,   478,   114,\n",
      "          2099,  2119,   193,    42,   478,   193,  2099,  3058,   246,   478,\n",
      "          2127, 10518, 26901,    51,  9665,   459,    42,   246,   478,    86,\n",
      "         22267,    33,  9665,   459,   655,   474,  2421,   764, 15652,   221,\n",
      "            93,   181,   155,   750,    88,  3599,  3070,   185,   576,  7985,\n",
      "         26910,  9313,  5827, 10768,   193],\n",
      "        [  233, 20151,  8844,  6802,    56,   501,   386,    67,   193,   213,\n",
      "          3278, 10236,   670,  2085,   776,  4053,   309,    30,    30,  5953,\n",
      "           177,  2664,   221,    50,   471,  3173,   459,   459,  3953,   167,\n",
      "            50,   471,  3173,   459,  2013, 15480,    32,   437,  6802,  6783,\n",
      "          2774,    56,   501,   173,  2617,    50,  2617,  3173,   185,   155,\n",
      "         15598,    30,   193,   478,  1346,  6360,   579,  5152,  3939, 15598,\n",
      "         26898,    50,  2319,   193,   478,  1346,  6360,   579,   987, 15598,\n",
      "         26898,  1387,   225,    30,  1138],\n",
      "        [  233,  3275,    93,   194,   149,   746,  1302,  1616, 23554,  1319,\n",
      "           348,   386,   229,  1302,   149,  1829, 13018,  1671,   958, 26904,\n",
      "          1169, 13051,  3277,   221,    93,    53, 20562, 26902,   295,   127,\n",
      "          1575,  1567,  1319,    68,   181,   961,   221,    93,    53, 20562,\n",
      "         26902, 24314, 10656,  1462,   478,  3620, 13018,   114,  2118,   170,\n",
      "          2118,   170,  2118, 14769,  1169,  3667,   149, 11081, 12482,   178,\n",
      "           787,  3828,   149, 12482,   229,   479,   194,   167,    88,  1350,\n",
      "            37, 13018,  1350,  2118,   388],\n",
      "        [  304,    86, 24383, 26898,   127,    93,  2423,  9933,  3296,  4304,\n",
      "         10894,   472,    81,   474,   246,    30,  5953,    93,   414, 16314,\n",
      "           474,   926,    91, 18334,  2625,   185,    93,   414,  2196,    57,\n",
      "         12767,   185,    39,    25, 12130,  1865,   149,  1757,   155,   560,\n",
      "            39,    25, 12130,   519,   874, 20984,   733,   516,    65,   287,\n",
      "          9013,  1365,   261,   620, 21999, 26898,   229,   185,  2013,  7066,\n",
      "          1882,    30, 10987,  1724, 23198,    30,  6381,   193, 10020, 16246,\n",
      "           142,    93,  1856,  3734,    93]]), tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1.]]), tensor([[59, 59, 27, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59,  6, 59,\n",
      "         59, 59, 59, 59, 59,  6, 59, 59, 59,  6, 59, 59, 59, 59, 59, 59, 59, 59,\n",
      "         59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59,\n",
      "         59, 59, 59, 59, 28, 59, 28, 59, 59, 59, 59, 59, 59,  6, 35, 59, 59, 59,\n",
      "         59, 59, 59],\n",
      "        [59, 28, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59,\n",
      "         59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59,\n",
      "         11, 59, 59, 59, 28, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59,\n",
      "         59, 59, 59, 59,  6, 35, 59, 59, 59, 59, 59, 59, 59, 28, 59, 59, 59,  6,\n",
      "         35, 59, 59],\n",
      "        [59, 59, 59, 28, 59,  5, 59, 59, 59, 59, 59, 59, 59, 59, 59,  1, 59, 59,\n",
      "         59, 59, 59, 59, 59, 28, 59, 59, 59, 59, 59, 59, 59, 11, 59, 59, 59, 59,\n",
      "         59, 59, 59, 20, 59, 59, 18, 59, 59, 59, 59, 59, 59, 59, 59, 28, 59, 59,\n",
      "         59, 59, 28, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59,\n",
      "         59, 59, 11],\n",
      "        [59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59,  2, 59,  2, 59,\n",
      "         59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 11, 59, 59, 59, 59, 59,  2, 59,\n",
      "         59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59,\n",
      "         59, 59, 59, 59, 59, 59, 59, 59,  6, 35, 59, 59, 59, 59, 59, 59, 59, 59,\n",
      "         59, 59, 59],\n",
      "        [59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59,\n",
      "         59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59,\n",
      "         59, 59, 59, 21, 59, 59, 59, 59, 59, 59, 11, 59, 59, 59, 59, 59, 59, 59,\n",
      "         59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59,\n",
      "         59, 59, 59],\n",
      "        [59, 59, 59, 12, 59, 59, 59, 12, 59, 59, 21, 59, 59, 59, 59, 19, 59, 59,\n",
      "         59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59,\n",
      "         59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59,\n",
      "         59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59,\n",
      "         59,  5, 34],\n",
      "        [59, 12, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 28, 59, 59,\n",
      "         59, 59, 59, 59,  5, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59,\n",
      "         59, 59, 59, 21, 59, 59, 59, 59, 59, 59, 21, 59, 59, 59, 59, 28, 59, 59,\n",
      "         59, 59, 59, 59, 59, 59,  2, 32, 59,  2, 32, 32, 28, 59, 59, 59, 59, 59,\n",
      "         59, 59,  2],\n",
      "        [59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 28, 59, 59, 59, 59, 18, 59, 59,\n",
      "         59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59,\n",
      "         59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59,\n",
      "         11, 59, 59, 59, 59, 59, 59, 59, 59, 59,  5, 59, 59, 59, 59, 59, 59, 59,\n",
      "         59, 59, 59],\n",
      "        [59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 16, 59, 59, 59,\n",
      "         59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59,\n",
      "         59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59,\n",
      "         59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59,\n",
      "         59, 59, 59],\n",
      "        [59, 59, 18, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 18, 59, 59,\n",
      "         59, 59,  5, 34, 34, 34, 59, 59, 59, 59, 11, 59, 59, 59, 59, 59, 59, 59,\n",
      "         59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 18, 59,\n",
      "         59, 18, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 18, 59,\n",
      "         18, 59, 59]])]\n"
     ]
    }
   ],
   "source": [
    "for step, batch in enumerate(train_dataloader):\n",
    "    print(step)\n",
    "    print(\"\")\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30000, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=60, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize model\n",
    "model = BertForTokenClassification.from_pretrained(BERT_MODEL, num_labels=len(tag2idx))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized optimizer and set hyperparameters.\n"
     ]
    }
   ],
   "source": [
    "# Set hyperparameters (optimizer, weight decay, learning rate)\n",
    "optimizer_grouped_parameters = get_hyperparameters(model, FULL_FINETUNING)\n",
    "optimizer = Adam(optimizer_grouped_parameters, lr=3e-5)\n",
    "print(\"Initialized optimizer and set hyperparameters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training loop.\n",
      "Train loss: 2.7238752047220864\n",
      "Starting validation loop.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chinchen/opt/anaconda3/envs/venv/lib/python3.7/site-packages/ipykernel_launcher.py:46: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch:  25%|██▌       | 1/4 [00:24<01:12, 24.11s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 1.0420911312103271\n",
      "Validation Accuracy: 0.0\n",
      "Classification Report:\n",
      "            precision    recall  f1-score   support\n",
      "\n",
      "      NRP       0.00      0.00      0.00         3\n",
      "      GPE       0.00      0.00      0.00        30\n",
      "      DUR       0.00      0.00      0.00         8\n",
      "      LOC       0.00      0.00      0.00         1\n",
      "     TIME       0.00      0.00      0.00        12\n",
      "      AGE       0.00      0.00      0.00         3\n",
      "     DATE       0.00      0.00      0.00         8\n",
      "      MON       0.00      0.00      0.00         1\n",
      "     FREQ       0.00      0.00      0.00         2\n",
      "      LAN       0.00      0.00      0.00         4\n",
      "      PER       0.00      0.00      0.00         1\n",
      "     SORD       0.00      0.00      0.00         1\n",
      "\n",
      "micro avg       0.00      0.00      0.00        74\n",
      "macro avg       0.00      0.00      0.00        74\n",
      "\n",
      "Confusion Matrix:\n",
      " \tB-AGE B-DATE B-DUR B-FREQ B-GPE B-LAN B-LOC B-MON B-NRP B-PER B-SORD B-TIME I-AGE I-DATE I-DUR I-FREQ I-GPE I-MON I-SORD O\n",
      "B-AGE\t[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "B-DATE\t[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 7]\n",
      "B-DUR\t[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 8]\n",
      "B-FREQ\t[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "B-GPE\t[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 30]\n",
      "B-LAN\t[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4]\n",
      "B-LOC\t[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "B-MON\t[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "B-NRP\t[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "B-PER\t[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "B-SORD\t[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "B-TIME\t[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 12]\n",
      "I-AGE\t[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "I-DATE\t[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "I-DUR\t[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "I-FREQ\t[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "I-GPE\t[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "I-MON\t[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "I-SORD\t[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "O\t[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0 592]\n",
      "F1-Score: 0.8770370370370371\n",
      "Train loss: 0.9410602649052938\n",
      "Starting validation loop.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch:  50%|█████     | 2/4 [00:51<00:50, 25.23s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.8073590993881226\n",
      "Validation Accuracy: 0.0\n",
      "Classification Report:\n",
      "            precision    recall  f1-score   support\n",
      "\n",
      "      NRP       0.00      0.00      0.00         3\n",
      "      GPE       0.00      0.00      0.00        30\n",
      "      DUR       0.00      0.00      0.00         8\n",
      "      LOC       0.00      0.00      0.00         1\n",
      "     TIME       0.00      0.00      0.00        12\n",
      "      AGE       0.00      0.00      0.00         3\n",
      "     DATE       0.00      0.00      0.00         8\n",
      "      MON       0.00      0.00      0.00         1\n",
      "     FREQ       0.00      0.00      0.00         2\n",
      "      LAN       0.00      0.00      0.00         4\n",
      "      PER       0.00      0.00      0.00         1\n",
      "     SORD       0.00      0.00      0.00         1\n",
      "\n",
      "micro avg       0.00      0.00      0.00        74\n",
      "macro avg       0.00      0.00      0.00        74\n",
      "\n",
      "Confusion Matrix:\n",
      " \tB-AGE B-DATE B-DUR B-FREQ B-GPE B-LAN B-LOC B-MON B-NRP B-PER B-SORD B-TIME I-AGE I-DATE I-DUR I-FREQ I-GPE I-MON I-SORD O\n",
      "B-AGE\t[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "B-DATE\t[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 7]\n",
      "B-DUR\t[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 8]\n",
      "B-FREQ\t[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "B-GPE\t[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 30]\n",
      "B-LAN\t[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4]\n",
      "B-LOC\t[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "B-MON\t[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "B-NRP\t[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "B-PER\t[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "B-SORD\t[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "B-TIME\t[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 12]\n",
      "I-AGE\t[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "I-DATE\t[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "I-DUR\t[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "I-FREQ\t[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "I-GPE\t[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "I-MON\t[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "I-SORD\t[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "O\t[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0 592]\n",
      "F1-Score: 0.8770370370370371\n",
      "Train loss: 0.8578117688496908\n",
      "Starting validation loop.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch:  75%|███████▌  | 3/4 [01:22<00:26, 26.87s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.7267888784408569\n",
      "Validation Accuracy: 0.0\n",
      "Classification Report:\n",
      "            precision    recall  f1-score   support\n",
      "\n",
      "      NRP       0.00      0.00      0.00         3\n",
      "      GPE       0.00      0.00      0.00        30\n",
      "      DUR       0.00      0.00      0.00         8\n",
      "      LOC       0.00      0.00      0.00         1\n",
      "     TIME       0.00      0.00      0.00        12\n",
      "      AGE       0.00      0.00      0.00         3\n",
      "     DATE       0.00      0.00      0.00         8\n",
      "      MON       0.00      0.00      0.00         1\n",
      "     FREQ       0.00      0.00      0.00         2\n",
      "      LAN       0.00      0.00      0.00         4\n",
      "      PER       0.00      0.00      0.00         1\n",
      "     SORD       0.00      0.00      0.00         1\n",
      "\n",
      "micro avg       0.00      0.00      0.00        74\n",
      "macro avg       0.00      0.00      0.00        74\n",
      "\n",
      "Confusion Matrix:\n",
      " \tB-AGE B-DATE B-DUR B-FREQ B-GPE B-LAN B-LOC B-MON B-NRP B-PER B-SORD B-TIME I-AGE I-DATE I-DUR I-FREQ I-GPE I-MON I-SORD O\n",
      "B-AGE\t[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "B-DATE\t[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 7]\n",
      "B-DUR\t[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 8]\n",
      "B-FREQ\t[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "B-GPE\t[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 30]\n",
      "B-LAN\t[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4]\n",
      "B-LOC\t[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "B-MON\t[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "B-NRP\t[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "B-PER\t[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "B-SORD\t[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "B-TIME\t[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 12]\n",
      "I-AGE\t[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "I-DATE\t[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "I-DUR\t[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "I-FREQ\t[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "I-GPE\t[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "I-MON\t[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "I-SORD\t[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "O\t[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0 592]\n",
      "F1-Score: 0.8770370370370371\n",
      "Train loss: 0.8336925109227499\n",
      "Starting validation loop.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 100%|██████████| 4/4 [01:49<00:00, 27.39s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.7235440015792847\n",
      "Validation Accuracy: 0.0\n",
      "Classification Report:\n",
      "            precision    recall  f1-score   support\n",
      "\n",
      "      NRP       0.00      0.00      0.00         3\n",
      "      GPE       0.00      0.00      0.00        30\n",
      "      DUR       0.00      0.00      0.00         8\n",
      "      LOC       0.00      0.00      0.00         1\n",
      "     TIME       0.00      0.00      0.00        12\n",
      "      AGE       0.00      0.00      0.00         3\n",
      "     DATE       0.00      0.00      0.00         8\n",
      "      MON       0.00      0.00      0.00         1\n",
      "     FREQ       0.00      0.00      0.00         2\n",
      "      LAN       0.00      0.00      0.00         4\n",
      "      PER       0.00      0.00      0.00         1\n",
      "     SORD       0.00      0.00      0.00         1\n",
      "\n",
      "micro avg       0.00      0.00      0.00        74\n",
      "macro avg       0.00      0.00      0.00        74\n",
      "\n",
      "Confusion Matrix:\n",
      " \tB-AGE B-DATE B-DUR B-FREQ B-GPE B-LAN B-LOC B-MON B-NRP B-PER B-SORD B-TIME I-AGE I-DATE I-DUR I-FREQ I-GPE I-MON I-SORD O\n",
      "B-AGE\t[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "B-DATE\t[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 7]\n",
      "B-DUR\t[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 8]\n",
      "B-FREQ\t[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "B-GPE\t[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 30]\n",
      "B-LAN\t[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4]\n",
      "B-LOC\t[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "B-MON\t[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "B-NRP\t[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "B-PER\t[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "B-SORD\t[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "B-TIME\t[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 12]\n",
      "I-AGE\t[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "I-DATE\t[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "I-DUR\t[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "I-FREQ\t[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "I-GPE\t[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "I-MON\t[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "I-SORD\t[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "O\t[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0 592]\n",
      "F1-Score: 0.8770370370370371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Start fine-tuning model, set epochs and max_grad_norm\n",
    "epochs = 4\n",
    "max_grad_norm = 1.0\n",
    "\n",
    "print(\"Starting training loop.\")\n",
    "epoch = 0\n",
    "for _ in trange(epochs, desc=\"Epoch\"):\n",
    "    epoch += 1\n",
    "\n",
    "    model.train()\n",
    "    tr_loss, tr_accuracy = 0, 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    tr_preds, tr_labels = [], []\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # add batch to gpus\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(\n",
    "            b_input_ids,\n",
    "            token_type_ids=None,\n",
    "            attention_mask=b_input_mask,\n",
    "            labels=b_labels,\n",
    "        )\n",
    "        loss, tr_logits = outputs[:2]\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Compute train loss\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "\n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            parameters=model.parameters(), max_norm=max_grad_norm\n",
    "        )\n",
    "\n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "        model.zero_grad()\n",
    "\n",
    "    # print train loss per epoch\n",
    "    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "\n",
    "\n",
    "    # Validation loop\n",
    "    print(\"Starting validation loop.\")\n",
    "\n",
    "    model.eval()\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    predictions, true_labels = [], []\n",
    "\n",
    "    for batch in valid_dataloader:\n",
    "\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(\n",
    "                b_input_ids,\n",
    "                token_type_ids=None,\n",
    "                attention_mask=b_input_mask,\n",
    "                labels=b_labels,\n",
    "            )\n",
    "            tmp_eval_loss, logits = outputs[:2]\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
    "        true_labels.append(label_ids)\n",
    "\n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "\n",
    "        eval_loss += tmp_eval_loss.mean().item()\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "        nb_eval_examples += b_input_ids.size(0)\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "\n",
    "    # Evakuate loss, acc, conf. matrix and report on dev set.\n",
    "    pred_tags = [idx2tag[p_i] for p in predictions for p_i in p]\n",
    "    valid_tags = [idx2tag[l_li] for l in true_labels[0] for l_li in l]\n",
    "    cl_report = classification_report(valid_tags, pred_tags)\n",
    "    conf_mat = annot_confusion_matrix(valid_tags, pred_tags)\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    eval_accuracy = eval_accuracy / nb_eval_steps\n",
    "\n",
    "    # Report metrics\n",
    "    print(\"Validation loss: {}\".format(eval_loss))\n",
    "    print(\"Validation Accuracy: {}\".format(eval_accuracy))\n",
    "    print(\"Classification Report:\\n {}\".format(cl_report))\n",
    "    print(\"Confusion Matrix:\\n {}\".format(conf_mat))\n",
    "    print(\"F1-Score: {}\".format(flat_accuracy(pred_tags, valid_tags)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
